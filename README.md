- - - # **Differences Between Cloud Data Engineering and Data Engineering**  
	## Cloud Data Engineering	
1	Cloud engineers design data infrastructure for organizations	Data engineers are responsible for the development, integrity, and maintenance of an organization’s data infrastructure.
2	Cloud data engineering focuses on managing different cloud services to provide an integrated package to customers according to their requirements.	The goal of data engineering is to ensure a solid foundation of data for use in business-critical analytics or reporting.  
3	A cloud data engineer analyzes the customer’s needs, the amount and type of data they have, and the result of their operations. They also decide the best tools and services so that customers can use them to have optimal performance.	Data engineers ensure that data has been properly stored, cleansed, or integrated and it is fundamental to all organizational data initiatives. Everyone wants cleaner data. Data Integrity and Quality issues rarely stem from tool choice, however. They usually stem from business priorities or disparate data sources.  
4	Cloud data engineering manages data pipelines, data transfer, and data storage.	Data engineers might work in the data science domain on machine learning projects, for example, or work closely with business users on an analytics project. In this way, data engineers serve as a critical data partner for just about all organizational functions.  
5		Given a data engineer’s breadth of knowledge, their responsibilities will often overlap with business analysts, data scientists, and data architects roles.  
6		A data engineer understands wide range of data systems and platforms, such as data warehouses and data lakes, relational databases, such as MySQL and PostgreSQL, and NoSQL databases and Apache Spark systems.  

## Data Engineering  
* Data engineers are responsible for the development, integrity, and maintenance of an organization’s data infrastructure.  
* The goal of data engineering is to ensure a solid foundation of data for use in business-critical analytics or reporting.
* Data engineers ensure that data has been properly stored, cleansed, or integrated and it is fundamental to all organizational data initiatives. Everyone wants cleaner data. Data Integrity and Quality issues rarely stem from tool choice, however. They usually stem from business priorities or disparate data sources.  
* Data engineers might work in the data science domain on machine learning projects, for example, or work closely with business users on an analytics project. In this way, data engineers serve as a critical data partner for just about all organizational functions.  
* Given a data engineer’s breadth of knowledge, their responsibilities will often overlap with business analysts, data scientists, and data architects roles.  
* A data engineer understands wide range of data systems and platforms, such as data warehouses and data lakes, relational databases, such as MySQL and PostgreSQL, and NoSQL databases and Apache Spark systems.  


## **Tools used by Cloud Data Engineering**
* Data ingestions tools – Amazon Kinesis Firehose, AWS Snowball, AWS Storage Gateway   
* Data storage tools – Amazon S3, AWS Athena  
* Data integration tools – AWS Glue  
* Data warehouse tools – Amazon Redshift  
* Data visualization tools – Amazon QuickSight  
* Others are Hevo Data - Hevo Data, a No-code Data Pipeline helps automates integration of data from 100+ sources (including 40+ free sources) and load it in a data warehouse of your choice to visualize it in your desired BI tool.	
## **Tools uded by Data Engineering**
* Python and Scala  
* R and   
* SQL,   
* Others are NoSQL, Hadoop, Spark, Apache, MondoDB  
* Familiarity with different operating systems like Linux, Windows and so on  
* Experience with ETL/ELT tools and REST-oriented APIs, which are used for data integration  

# **Some Software Engineering Practices for Data Engineers**  
* Clean codes: Write simple, concise, and readable codes  
* Modular codes: This means that the code is broken into functions and modules  
* Minimize number of entities (functions, classes): make functions or classes only if it is necessary.  
* Function should do one thing  
* Use module: If you have several functions or classes that you used in several files, try writing them in a separate file and import them in other files where you need it. This keeps your code short, maintainable, and reusable.  
* Refactoring  
* Documentation  
* Versioning  
* Logging: Logging is a valuable tool that records the events and errors that occurred while running the program  
* Testing   
* Enable your pipeline to handle concurrent workloads  
* Tap into existing skills to get the job done  
* Use data streaming instead of batch ingestion  
* Streamline pipeline development processes  
* Operationalize pipeline development: Follow Data Ops  
* Invest in tools with built-in connectivity  
* Incorporate extensibility: By using APIs and pipelining tools, you can create a data flow that uses outside code seamlessly.  
* Enable data sharing in your pipelines: Data infrastructure code should all live in the same place. At least, code dealing with Automation, extraction, and finally loading. This eliminates data silos.  
* Choose the right tool for data wrangling: Everyone wants cleaner data. Data integrity and quality issues rarely stem from tool choice, however. They usually stem from business priorities or disparate data sources.  
* Build data cataloging into your engineering strategy  
* Rely on data owners to set security policy  